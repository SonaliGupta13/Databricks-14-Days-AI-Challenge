{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68afd393-0046-440a-a82e-11e9af232661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load silver events\n",
    "events = spark.table(\"ecommerce.silver.events\")\n",
    "\n",
    "# Simple feature set\n",
    "ml_data = (\n",
    "    events\n",
    "    .filter(F.col(\"price\").isNotNull())\n",
    "    .withColumn(\"hour\", F.hour(\"event_time\"))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"event_time\"))\n",
    "    .select(\"price\", \"hour\", \"day_of_week\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b5dd41-546e-481a-a44e-1fd159ca03c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Vectorize features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"day_of_week\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "final_data = assembler.transform(ml_data).select(\"features\", \"price\")\n",
    "\n",
    "## Train-test split\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Train Linear Regression model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"price\")\n",
    "\n",
    "model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c2864b-c4d5-417f-94b9-54b45d14835e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/Volumes/workspace/default/kaggle_volume/mlflow_tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "541b28fd-eaa3-4198-81f7-832c46f8a58d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/default/kaggle_volume/mlflow_tmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf15a0f1-92cd-4103-b21c-27de0bf115ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/20 07:07:21 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/20 07:07:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-831b0ef4-205c-49ee-b27d-13/tmp7_fw6eg7/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/20 07:07:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 357.99881212509115\n"
     ]
    }
   ],
   "source": [
    "## Log parameters, metrics, model\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "# Required for Unity Catalog\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/default/kaggle_volume/mlflow_tmp\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"features\", \"hour, day_of_week\")\n",
    "\n",
    "    # Predictions\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"price\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\" \n",
    "    )\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Log metric\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.spark.log_model(model, \"linear_regression_model\")\n",
    "\n",
    "    print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e6ae09-cd30-4685-9b43-914002d76d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Compare Runs\n",
    "\n",
    "## Simple Features\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"features\", \"hour, day_of_week\")\n",
    "    mlflow.log_param(\"model\", \"LinearRegression\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "## Add more features\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"features\", \"hour, day_of_week, is_weekend\")\n",
    "    mlflow.log_param(\"model\", \"LinearRegression\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
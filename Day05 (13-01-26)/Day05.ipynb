{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c1af1e-b110-4ba4-8955-4af4cbf1d851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Reference existing Delta table by path\n",
    "delta_path = \"dbfs:/Volumes/workspace/default/kaggle_volume/delta/events\"\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a0887e-1d92-4edb-b4df-bf0a52501a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create incremental data\n",
    "updates = (\n",
    "    spark.read.format(\"delta\").load(delta_path)\n",
    "    .sample(0.02, seed=42)          # small batch\n",
    "    .withColumn(\"price\", F.col(\"price\") + 5)  # simulate change\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e605959-68a7-4575-a166-8eb6463e57cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate source\n",
    "updates_clean = updates.dropDuplicates(\n",
    "    [\"user_id\", \"event_time\", \"product_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e5bd00-b437-49f2-b438-e28e103f5a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental MERGE completed\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "deltaTable.alias(\"t\").merge(\n",
    "    updates_clean.alias(\"s\"),\n",
    "    \"\"\"\n",
    "    t.user_id = s.user_id\n",
    "    AND t.event_time = s.event_time\n",
    "    AND t.product_id = s.product_id\n",
    "    \"\"\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"Incremental MERGE completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4073621-8334-43f4-b5d7-fa4feda8f010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>0</td><td>2026-01-12T05:37:49.000Z</td><td>72099111605464</td><td>sonali.gupta@vensysco.in</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>null</td><td>List(1556992834470568)</td><td>0112-050619-8lqz2v4h-v2n</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 43, numRemovedFiles -> 0, numRemovedBytes -> 0, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1405244778)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "2026-01-12T05:37:49.000Z",
         "72099111605464",
         "sonali.gupta@vensysco.in",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         null,
         [
          "1556992834470568"
         ],
         "0112-050619-8lqz2v4h-v2n",
         null,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "43",
          "numOutputBytes": "1405244778",
          "numOutputRows": "42448764",
          "numRemovedBytes": "0",
          "numRemovedFiles": "0"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "userId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "userName",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "operation",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "operationParameters",
            "nullable": true,
            "type": {
             "keyType": "string",
             "type": "map",
             "valueContainsNull": true,
             "valueType": "string"
            }
           },
           {
            "metadata": {},
            "name": "job",
            "nullable": true,
            "type": {
             "fields": [
              {
               "metadata": {},
               "name": "jobId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobName",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobRunId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "runId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobOwnerId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "triggerType",
               "nullable": true,
               "type": "string"
              }
             ],
             "type": "struct"
            }
           },
           {
            "metadata": {},
            "name": "notebook",
            "nullable": true,
            "type": {
             "fields": [
              {
               "metadata": {},
               "name": "notebookId",
               "nullable": true,
               "type": "string"
              }
             ],
             "type": "struct"
            }
           },
           {
            "metadata": {},
            "name": "clusterId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "readVersion",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "isolationLevel",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isBlindAppend",
            "nullable": true,
            "type": "boolean"
           },
           {
            "metadata": {},
            "name": "operationMetrics",
            "nullable": true,
            "type": {
             "keyType": "string",
             "type": "map",
             "valueContainsNull": true,
             "valueType": "string"
            }
           },
           {
            "metadata": {},
            "name": "userMetadata",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "engineInfo",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"jobId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobRunId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"runId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobOwnerId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"triggerType\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"notebookId\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "-- Query Historical Versions (Time Travel)\n",
    "-- step1- View table history\n",
    "DESCRIBE HISTORY workspace.default.events_table;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72f1e5f9-3c67-43fb-ba6e-284f9239ca8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0 rows: 42448764\n"
     ]
    }
   ],
   "source": [
    "# step2- Read an old version\n",
    "v0 = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(\"Version 0 rows:\", v0.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b87e6e-b97d-4417-af2a-884e52203567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7382243017409246>, line 6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# step3- Read by timestamp\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m old_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestampAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-01\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mload(delta_path)\n",
       "\u001B[0;32m----> 6\u001B[0m old_data\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    865\u001B[0m             },\n",
       "\u001B[1;32m    866\u001B[0m         )\n",
       "\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n",
       "\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n",
       "\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n",
       "\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n",
       "\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n",
       "\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n",
       "\u001B[1;32m    874\u001B[0m     ),\n",
       "\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n",
       "\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n",
       "\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n",
       "\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m   1892\u001B[0m     )\n",
       "\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n",
       "\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: The provided timestamp (2024-01-01 00:00:00.0) is before the earliest version available to this\n",
       "table (2026-01-12 05:17:26.0). Please use a timestamp after 2026-01-12 05:17:26.\n",
       "         \n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.transaction.tahoe.DeltaErrorsBase$TimestampEarlierThanCommitRetentionException\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:364)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:788)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$3(DeltaTableV2.scala:381)\n",
       "\tat scala.Option.map(Option.scala:242)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:374)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:428)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:966)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:372)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:370)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:595)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:593)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:592)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2198)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:217)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n",
       "\tat scala.Option.map(Option.scala:242)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "The provided timestamp (2024-01-01 00:00:00.0) is before the earliest version available to this\ntable (2026-01-12 05:17:26.0). Please use a timestamp after 2026-01-12 05:17:26.\n         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaErrorsBase$TimestampEarlierThanCommitRetentionException\n\tat com.databricks.sql.transaction.tahoe.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:364)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:788)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$3(DeltaTableV2.scala:381)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:374)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:428)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:966)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:372)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:370)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:593)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:592)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2198)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:217)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: The provided timestamp (2024-01-01 00:00:00.0) is before the earliest version available to this\ntable (2026-01-12 05:17:26.0). Please use a timestamp after 2026-01-12 05:17:26.\n         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaErrorsBase$TimestampEarlierThanCommitRetentionException\n\tat com.databricks.sql.transaction.tahoe.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:364)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:788)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$3(DeltaTableV2.scala:381)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:374)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:428)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:966)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:372)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:370)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:593)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:592)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2198)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:217)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "com.databricks.sql.transaction.tahoe.DeltaErrorsBase$TimestampEarlierThanCommitRetentionException\n\tat com.databricks.sql.transaction.tahoe.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:364)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:788)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$3(DeltaTableV2.scala:381)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:374)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:428)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:966)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:372)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:370)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:593)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:592)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2198)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:217)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7382243017409246>, line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# step3- Read by timestamp\u001B[39;00m\n\u001B[1;32m      2\u001B[0m old_data \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestampAsOf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2024-01-01\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39mload(delta_path)\n\u001B[0;32m----> 6\u001B[0m old_data\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1123\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1123\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:876\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    861\u001B[0m             errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    862\u001B[0m             messageParameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    865\u001B[0m             },\n\u001B[1;32m    866\u001B[0m         )\n\u001B[1;32m    868\u001B[0m table, _ \u001B[38;5;241m=\u001B[39m DataFrame(\n\u001B[1;32m    869\u001B[0m     plan\u001B[38;5;241m.\u001B[39mShowString(\n\u001B[1;32m    870\u001B[0m         child\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan,\n\u001B[1;32m    871\u001B[0m         num_rows\u001B[38;5;241m=\u001B[39mn,\n\u001B[1;32m    872\u001B[0m         truncate\u001B[38;5;241m=\u001B[39m_truncate,\n\u001B[1;32m    873\u001B[0m         vertical\u001B[38;5;241m=\u001B[39mvertical,\n\u001B[1;32m    874\u001B[0m     ),\n\u001B[1;32m    875\u001B[0m     session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session,\n\u001B[0;32m--> 876\u001B[0m )\u001B[38;5;241m.\u001B[39m_to_table()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m   1892\u001B[0m     )\n\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: The provided timestamp (2024-01-01 00:00:00.0) is before the earliest version available to this\ntable (2026-01-12 05:17:26.0). Please use a timestamp after 2026-01-12 05:17:26.\n         \n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.DeltaErrorsBase$TimestampEarlierThanCommitRetentionException\n\tat com.databricks.sql.transaction.tahoe.DeltaHistoryManager.getActiveCommitAtTime(DeltaHistoryManager.scala:364)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.resolveTimeTravelVersion(DeltaTable.scala:788)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$3(DeltaTableV2.scala:381)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$2(DeltaTableV2.scala:374)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:428)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:966)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:372)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:370)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:187)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:593)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:592)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:722)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:722)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:721)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:471)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:694)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:674)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:674)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:666)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:437)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:955)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:948)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:945)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:944)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:932)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:943)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:942)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:419)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:418)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:392)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1123)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1123)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2198)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:217)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:867)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:191)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:191)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:191)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:842)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:790)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:231)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:219)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:197)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:197)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# step3- Read by timestamp\n",
    "old_data = spark.read.format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-01\") \\\n",
    "    .load(delta_path)\n",
    "\n",
    "old_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a69726-d535-4c10-9400-3babb63c830b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td></td><td>List(26, 43, List(30174253, 56176753, 4.4598923115384616E7, 26, 1159572001), List(5827713, 35958801, 3.268011111627907E7, 43, 1405244778), 0, List(minCubeSize(107374182400), List(0, 0), List(43, 1405244778), 0, List(43, 1405244778), 1, null), null, 0, 1, 43, 0, false, 0, 0, 1768284401303, 1768284420459, 8, 1, null, List(0, 0), null, 9, 9, 64142, 0, null)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "",
         [
          26,
          43,
          [
           30174253,
           56176753,
           4.4598923115384616E7,
           26,
           1159572001
          ],
          [
           5827713,
           35958801,
           3.268011111627907E7,
           43,
           1405244778
          ],
          0,
          [
           "minCubeSize(107374182400)",
           [
            0,
            0
           ],
           [
            43,
            1405244778
           ],
           0,
           [
            43,
            1405244778
           ],
           1,
           null
          ],
          null,
          0,
          1,
          43,
          0,
          false,
          0,
          0,
          1768284401303,
          1768284420459,
          8,
          1,
          null,
          [
           0,
           0
          ],
          null,
          9,
          9,
          64142,
          0,
          null
         ]
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "path",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "metrics",
            "nullable": true,
            "type": {
             "fields": [
              {
               "metadata": {},
               "name": "numFilesAdded",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "numFilesRemoved",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "filesAdded",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "min",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "max",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "avg",
                  "nullable": false,
                  "type": "double"
                 },
                 {
                  "metadata": {},
                  "name": "totalFiles",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSize",
                  "nullable": false,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "filesRemoved",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "min",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "max",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "avg",
                  "nullable": false,
                  "type": "double"
                 },
                 {
                  "metadata": {},
                  "name": "totalFiles",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSize",
                  "nullable": false,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "partitionsOptimized",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "zOrderStats",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "strategyName",
                  "nullable": true,
                  "type": "string"
                 },
                 {
                  "metadata": {},
                  "name": "inputCubeFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "num",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "inputOtherFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "num",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "inputNumCubes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "mergedFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "num",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "numOutputCubes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "mergedNumCubes",
                  "nullable": true,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "clusteringStats",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "inputZCubeFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "numFiles",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "inputOtherFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "numFiles",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "inputNumZCubes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "mergedFiles",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "numFiles",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "size",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "numOutputZCubes",
                  "nullable": false,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "numBins",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "numBatches",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "totalConsideredFiles",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "totalFilesSkipped",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "preserveInsertionOrder",
               "nullable": false,
               "type": "boolean"
              },
              {
               "metadata": {},
               "name": "numFilesSkippedToReduceWriteAmplification",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "numBytesSkippedToReduceWriteAmplification",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "startTimeMs",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "endTimeMs",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "totalClusterParallelism",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "totalScheduledTasks",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "autoCompactParallelismStats",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "maxClusterActiveParallelism",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "minClusterActiveParallelism",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "maxSessionActiveParallelism",
                  "nullable": true,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "minSessionActiveParallelism",
                  "nullable": true,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "deletionVectorStats",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "numDeletionVectorsRemoved",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numDeletionVectorRowsRemoved",
                  "nullable": false,
                  "type": "long"
                 }
                ],
                "type": "struct"
               }
              },
              {
               "metadata": {},
               "name": "recompressionCodec",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "numTableColumns",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "numTableColumnsWithStats",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "totalTaskExecutionTimeMs",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "skippedArchivedFiles",
               "nullable": false,
               "type": "long"
              },
              {
               "metadata": {},
               "name": "clusteringMetrics",
               "nullable": true,
               "type": {
                "fields": [
                 {
                  "metadata": {},
                  "name": "sizeOfTableInBytesBeforeLazyClustering",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "isNewMetadataCreated",
                  "nullable": false,
                  "type": "boolean"
                 },
                 {
                  "metadata": {},
                  "name": "isPOTriggered",
                  "nullable": false,
                  "type": "boolean"
                 },
                 {
                  "metadata": {},
                  "name": "isFull",
                  "nullable": false,
                  "type": "boolean"
                 },
                 {
                  "metadata": {},
                  "name": "approxClusteringQuality",
                  "nullable": true,
                  "type": "double"
                 },
                 {
                  "metadata": {},
                  "name": "approxClusteringQualityPerColumn",
                  "nullable": true,
                  "type": {
                   "containsNull": false,
                   "elementType": "double",
                   "type": "array"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "approxClusteringCoverage",
                  "nullable": true,
                  "type": "double"
                 },
                 {
                  "metadata": {},
                  "name": "compactionType",
                  "nullable": true,
                  "type": "string"
                 },
                 {
                  "metadata": {},
                  "name": "numFilesSkippedWithoutStats",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numFilesClassifiedToIntermediateNodes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "sizeOfFilesClassifiedToIntermediateNodesInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "logicalSizeOfFilesClassifiedToIntermediateNodesInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numFilesClassifiedToLeafNodes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "sizeOfFilesClassifiedToLeafNodesInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "logicalSizeOfFilesClassifiedToLeafNodesInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numThreadsForClassifier",
                  "nullable": true,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "clusterThresholdStrategy",
                  "nullable": true,
                  "type": "string"
                 },
                 {
                  "metadata": {},
                  "name": "minFileSize",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "maxFileSize",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "nodeMinNumFilesToCompact",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numIdealFiles",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numIdealFilesWithTrimmedStringMaxValue",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numAddedFilesWithSameMinMaxOnClusteringColumns",
                  "nullable": true,
                  "type": {
                   "containsNull": false,
                   "elementType": "long",
                   "type": "array"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "numClusteringTasksPlanned",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "numClusteringTasksNotPlannedDueToPO",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "numCompactionTasksPlanned",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "numCompactionTasksPlannedUndoneDueToPO",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "numOptimizeBatchesPlanned",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "numLeafNodesExpanded",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numLeafNodesClustered",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numGetFilesForNodeCalls",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numSamplingJobs",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numLeafNodesCompacted",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numLeafNodesCompactedUndoneDueToPO",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numIntermediateNodesCompacted",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numIntermediateNodesCompactedUndoneDueToPO",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSizeOfDataToCompactInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSizeOfDataToCompactInBytesUndoneDueToPO",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalLogicalSizeOfDataToCompactInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numIntermediateNodesClustered",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "numFilesSkippedAfterExpansion",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSizeOfFilesSkippedAfterExpansionInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalLogicalSizeOfFilesSkippedAfterExpansionInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalSizeOfDataToRewriteInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "totalLogicalSizeOfDataToRewriteInBytes",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "timeMetrics",
                  "nullable": true,
                  "type": {
                   "fields": [
                    {
                     "metadata": {},
                     "name": "classifierTimeMs",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "optimizerTimeMs",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "metadataLoadTimeMs",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "totalGetFilesForNodeCallsTimeMs",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "totalSamplingTimeMs",
                     "nullable": false,
                     "type": "long"
                    },
                    {
                     "metadata": {},
                     "name": "metadataCreationTimeMs",
                     "nullable": false,
                     "type": "long"
                    }
                   ],
                   "type": "struct"
                  }
                 },
                 {
                  "metadata": {},
                  "name": "maxOptimizeBatchesInParallel",
                  "nullable": false,
                  "type": "long"
                 },
                 {
                  "metadata": {},
                  "name": "currentIteration",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "maxIterations",
                  "nullable": false,
                  "type": "integer"
                 },
                 {
                  "metadata": {},
                  "name": "clusteringStrategy",
                  "nullable": true,
                  "type": "string"
                 }
                ],
                "type": "struct"
               }
              }
             ],
             "type": "struct"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 16
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "metrics",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"numFilesAdded\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numFilesRemoved\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"filesAdded\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"min\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"max\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"avg\",\"nullable\":false,\"type\":\"double\"},{\"metadata\":{},\"name\":\"totalFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSize\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"filesRemoved\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"min\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"max\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"avg\",\"nullable\":false,\"type\":\"double\"},{\"metadata\":{},\"name\":\"totalFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSize\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"partitionsOptimized\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"zOrderStats\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"strategyName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"inputCubeFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"num\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"inputOtherFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"num\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"inputNumCubes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"mergedFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"num\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"numOutputCubes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"mergedNumCubes\",\"nullable\":true,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"clusteringStats\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"inputZCubeFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"numFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"inputOtherFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"numFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"inputNumZCubes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"mergedFiles\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"numFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"numOutputZCubes\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"numBins\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numBatches\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalConsideredFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalFilesSkipped\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"preserveInsertionOrder\",\"nullable\":false,\"type\":\"boolean\"},{\"metadata\":{},\"name\":\"numFilesSkippedToReduceWriteAmplification\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numBytesSkippedToReduceWriteAmplification\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"startTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"endTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalClusterParallelism\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalScheduledTasks\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"autoCompactParallelismStats\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"maxClusterActiveParallelism\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"minClusterActiveParallelism\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"maxSessionActiveParallelism\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"minSessionActiveParallelism\",\"nullable\":true,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"deletionVectorStats\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"numDeletionVectorsRemoved\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numDeletionVectorRowsRemoved\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"recompressionCodec\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"numTableColumns\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numTableColumnsWithStats\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalTaskExecutionTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"skippedArchivedFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"clusteringMetrics\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"sizeOfTableInBytesBeforeLazyClustering\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"isNewMetadataCreated\",\"nullable\":false,\"type\":\"boolean\"},{\"metadata\":{},\"name\":\"isPOTriggered\",\"nullable\":false,\"type\":\"boolean\"},{\"metadata\":{},\"name\":\"isFull\",\"nullable\":false,\"type\":\"boolean\"},{\"metadata\":{},\"name\":\"approxClusteringQuality\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"approxClusteringQualityPerColumn\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"approxClusteringCoverage\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"compactionType\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"numFilesSkippedWithoutStats\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numFilesClassifiedToIntermediateNodes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"sizeOfFilesClassifiedToIntermediateNodesInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"logicalSizeOfFilesClassifiedToIntermediateNodesInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numFilesClassifiedToLeafNodes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"sizeOfFilesClassifiedToLeafNodesInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"logicalSizeOfFilesClassifiedToLeafNodesInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numThreadsForClassifier\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"clusterThresholdStrategy\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"minFileSize\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"maxFileSize\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"nodeMinNumFilesToCompact\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numIdealFiles\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numIdealFilesWithTrimmedStringMaxValue\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numAddedFilesWithSameMinMaxOnClusteringColumns\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"long\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"numClusteringTasksPlanned\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"numClusteringTasksNotPlannedDueToPO\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"numCompactionTasksPlanned\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"numCompactionTasksPlannedUndoneDueToPO\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"numOptimizeBatchesPlanned\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"numLeafNodesExpanded\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numLeafNodesClustered\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numGetFilesForNodeCalls\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numSamplingJobs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numLeafNodesCompacted\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numLeafNodesCompactedUndoneDueToPO\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numIntermediateNodesCompacted\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numIntermediateNodesCompactedUndoneDueToPO\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSizeOfDataToCompactInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSizeOfDataToCompactInBytesUndoneDueToPO\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalLogicalSizeOfDataToCompactInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numIntermediateNodesClustered\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"numFilesSkippedAfterExpansion\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSizeOfFilesSkippedAfterExpansionInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalLogicalSizeOfFilesSkippedAfterExpansionInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSizeOfDataToRewriteInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalLogicalSizeOfDataToRewriteInBytes\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"timeMetrics\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"classifierTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"optimizerTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"metadataLoadTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalGetFilesForNodeCallsTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"totalSamplingTimeMs\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"metadataCreationTimeMs\",\"nullable\":false,\"type\":\"long\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"maxOptimizeBatchesInParallel\",\"nullable\":false,\"type\":\"long\"},{\"metadata\":{},\"name\":\"currentIteration\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"maxIterations\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"clusteringStrategy\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}}],\"type\":\"struct\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "-- OPTIMIZE & ZORDER\n",
    "OPTIMIZE workspace.default.events_table\n",
    "ZORDER BY (event_type, user_id);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a27df4-99b9-4162-ab0a-acfe8d49dda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         ""
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "path",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 19
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "-- Clean Old Files (VACUUM)\n",
    "VACUUM workspace.default.events_table RETAIN 168 HOURS;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7382243017409248,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day05",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}